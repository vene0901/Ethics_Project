<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Implications of AI</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

<header>
    <h1>Racial Bias in AI</h1>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="about.html">About Us</a></li>
            <li class="dropdown">
                <a class="dropbtn" href="#">Impacts</a>
                <div class="dropdown-content">
                    <a href="#ethnic_names">Ethnic Names & Ethnicity</a>
                    <a href="#race">Race</a>
                    <a href="#facial_recognition">Facial Recognition</a>
                </div>
            </li>
            <li><a href="personal_experience.html">Personal Experience</a></li>
            <li><a href="#">Tools</a></li>
            <li><a href="#">Personal Experience</a></li>
            <li><a href="#">Feedback</a></li>
        </ul>
    </nav>
</header>

<main>
    <section id="racial_bias" class="impact-section">
        <h2>Racial Bias in AI</h2>
        <p>Understanding the Impact: AI systems trained on racially biased data may perpetuate racial discrimination in various domains, including law enforcement, finance, and healthcare.</p>
        <p>Potential Consequences: Biased AI can lead to racial profiling, unequal access to opportunities, and exacerbation of existing social disparities.</p>
        <p>Data & Insights:</p>
        <p>ProPublica's investigation titled "Machine Bias: There's Software Used Across the Country to Predict Future Criminals. And It's Biased Against Blacks" revealed alarming racial bias in predictive policing algorithms. The report, published on May 23, 2016, shed light on how widely-used risk assessment tools, such as COMPAS, disproportionately label African American defendants as high risk, contributing to unjust sentencing disparities (ProPublica, 2016).</p>
        <p>Case study:</p>
        <p>In 2019, a study by ProPublica revealed that a widely used risk assessment tool in the U.S. criminal justice system, COMPAS, exhibited racial bias. African American defendants were more likely to be incorrectly labeled as high risk compared to white defendants. This biased AI tool led to unjust sentencing disparities and highlighted the urgent need for fair and accountable AI systems in law enforcement.</p>
    </section>
</main>

</body>
</html>
